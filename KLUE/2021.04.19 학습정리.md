# 학습정리

### 오늘 한 것

- mask 추가하여 학습

  - 효과가 다른 설정에 따라 달라지는 것 같다.
  - 최고 acc 보이는 모델에는 사용

- sentence switch

  - argumentation 효과를 줄려고 하였으나 큰 효과를 보지 못하였다.

    => 다른 설정으로 학습해보기 효과가 있을법한 방법인 것 같다...

  - 비효율적 으로 작성하였다고 생각했는데 생각보다 시간 소모 X

    => epoch당 약 1 ~ 2초 정도

- sentence split

  - 문장을 entity 기준으로 잘라서 학습
  - 내 모델에서는 효과가 없었다.

- LSTM 추가

  - Transformer 뒤에 lstm과 dense layer를 추가하여 학습

  - 최종 acc가 오히려 감소

    => 한번만 실험 해 보았기 때문에 다른 조건에서도 계속하여 실험 해보기

​            

### 시도해 볼 것

- 내일 피어세션 때 만들었던 data 사용 결과를 공유 받고 사용해보기
  - 데이터가 조금 늘어나면 stratified k fold를 사용 할 수 있지 않을까?
- Entity 끼리 연결하는 방법 변경해 보기
- LSTM 수정
- 아이디어 구상해보기...
- 시각화 강의 듣기