# 학습정리

**Dense embedding**

구절을 벡터로 변환

작은 차원의 고밀도 벡터

각 차원이 특정 term에 대응 X

Bert와 같은 Pretrained language model로학습



**similarity search**

모든 문서를 탐색하기에는 방대한 양의 데이터셋을 모두 탐색 할 수 없다
=> 정확도를 약간 희생하여 속도, 메모리 효율을 높인다.



compression : vector를 압축하여 메모리 사용량을 줄임, 정보의 손실이 발생한다.

Pruning : Search space를 줄여 속도를 계선한다. Clustering + inverted file



**FAISS**

FAISS라이브러리를사용하여 clustering, ivf등을 빨리 할 수있다.

​       

## 오늘 한 것

Dense retrieval 구현, 테스트

=> train data들을 가지고 학습후  vaildation으로 평가

=> validation데이터에 대하여는 아주 좋은 성능을 보였지만 wiki 데이터를 탐색하는곳에는 매우 안좋은성능을 보였다.

​         

잘 못 구현?

validation set은 약 240개 정도이고 wiki 데이터set은 약 50000개이다. 여기서오는 난이도 차이??

검증 방법을 변경해야하나?



## 해볼 것

검증방법 변경

잘못된 코드가 있는지 확인하기